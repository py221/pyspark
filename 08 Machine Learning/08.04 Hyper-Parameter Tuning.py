# Databricks notebook source
# MAGIC %md 
# MAGIC ###Machine Learning: Hyper-Parameter Tuning
# MAGIC 
# MAGIC **Objective:** The objective of this lab is to introduce you to the concept of hyper-parameter tuning.

# COMMAND ----------

# MAGIC %md Let's revisit our work with the K-Nearest Neighbor algorithm. We'll train a model to predict party affliation based on voting records, using an 80:20 train:test split as demonstrated in the last lab:

# COMMAND ----------

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# read the dataset to pandas df
df = (
  spark
    .read
    .csv(
      'wasbs://downloads@smithbc.blob.core.windows.net/party/house-votes-84.data',
      sep=',',
      header=True,
      inferSchema=True,
      nanValue='?'
      )
  ).toPandas()

# remove rows with missing votes & reset index on the dataframe
df = df.dropna().reset_index(drop=True)

# extract data for fitting
X = df.drop('party', axis=1).values  # features
y = df['party'].values # labels

# split dataset
X_train, X_test, y_train, y_test = train_test_split(
  X, y, 
  test_size = 0.2, 
  random_state=42, 
  stratify=y
  )

# train & test
knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(X_train, y_train)
score = knn.score(X_test, y_test)

# summarize results
print( score )

# COMMAND ----------

# MAGIC %md In this and all previous training runs of the KNN model, we've set k to a value of 6.  What this means is that the algorithm considers the 6 nearest neighbors in determining the value of a test point. But is this the right value?
# MAGIC 
# MAGIC Take a look at this illustration and consider the value we should assign to the data point identified in orange:
# MAGIC 
# MAGIC <img src="https://smithbc.blob.core.windows.net/downloads/misc/images/knn.png" width=600 />
# MAGIC 
# MAGIC It's neighbors are numbered 1, 2, 3, *etc.* based on their proximity to our orange dot.  If we set k to 1, we would predict our orange dot is blue.  If we set k to 2, we'd need to figure out how choose between a green or blue assignment.  Setting k to 3 would result in a green assignment as two of the three nearest neighbors are now green.
# MAGIC 
# MAGIC As the value of k increases, we consider more and more dots and the color assigned to our dot increasingly moves to reflect the overall balance between blue and green dots in the diagram.

# COMMAND ----------

# MAGIC %md The value k is what is known as a hyper-parameter.  A hyper-parameter is a parameter value that's used to affect the algorithm's behavior during training. Some hyper-parameters are best set leveraging a knowledge of the dataset or the probem space being addressed.  Others are best set by adjusting the hyper-parameter value over a reasonable range of potential values and evaulating how model metrics change.  This later technique is known as hyper-parameter tuning.
# MAGIC 
# MAGIC To explore hyper-parameter tuning, let's see how our model improves or degrades in its accuracy as we move through values of 1 through 20 for k:

# COMMAND ----------

# configure values for k
k_values = np.arange(1, 21)

# configure arrays to hold accuracy measures for each instance of k
train_accuracy = np.empty(len(k_values))
test_accuracy = train_accuracy.copy()

# train and evauluate for each k
for i, k in enumerate(k_values):
  
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    
    # score model on both train and test datasets
    train_accuracy[i] = knn.score(X_train, y_train)
    test_accuracy[i] = knn.score(X_test, y_test)

# COMMAND ----------

# MAGIC %md Now, we can plot the accuracy of our training set relative to the accuracy of our testing set:

# COMMAND ----------

import matplotlib.pyplot as plt

plt.title('k-NN: Varying Number of Neighbors')
plt.plot(k_values, test_accuracy, label = 'Testing Accuracy')
plt.plot(k_values, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')

display()

# COMMAND ----------

# MAGIC %md Notice how training and testing accuracy converge at critical values of k.  In this dataset, this occurs at k=2 and k=6.  We might consider using those values of k for this model.  
# MAGIC 
# MAGIC Notice too that this was generated by using a fixed value for the random seed on a split.  If we add cross-validation to our logic, we should be able to achieve some more reliable estimates of accuracy:

# COMMAND ----------

# MAGIC %md **NOTE** I will be using the KFolds class from the sklearn.model_selection library.  This will allow me to define my folds in advance and then iterate over those folds.  This will keep the splits consistent between each iteraction of the model, increasing the reliability of my results.

# COMMAND ----------

from sklearn.model_selection import KFold

# assign index values to splits
number_of_splits = 5
kf = KFold(n_splits=number_of_splits, random_state=42)

# configure values for k (in k-NN)
k_values = np.arange(1, 21)
train_accuracy = np.empty(len(k_values))
test_accuracy = np.empty(len(k_values))

# train and evauluate for each k
for i, k in enumerate(k_values):
  
  train_accuracy_accum = 0
  test_accuracy_accum = 0

  # for each fold
  for train, test in kf.split(X):
  
    # assign train and test
    X_train, X_test = X[train], X[test]
    y_train, y_test = y[train], y[test]

    # fit the model
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    
    # accumulate accuracy scores
    train_accuracy_accum += knn.score(X_train, y_train)
    test_accuracy_accum += knn.score(X_test, y_test)
    
  # average the accuracy scores for this value of k
  train_accuracy[i] = train_accuracy_accum/number_of_splits
  test_accuracy[i] = test_accuracy_accum/number_of_splits

# COMMAND ----------

import matplotlib.pyplot as plt

plt.title('k-NN: Varying Number of Neighbors')
plt.plot(k_values, test_accuracy, label = 'Testing Accuracy')
plt.plot(k_values, train_accuracy, label = 'Training Accuracy')
plt.legend()
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')

display()

# COMMAND ----------

# MAGIC %md When training models, there is often no one right value for a hyper-parameter and values that may have been optimal at one point in time may no longer be so as the data changes.  A common practice in Data Science is to employ hyper-parameter optimization with each evolution of the dataset on which the model is being trained and retrained.

# COMMAND ----------

# MAGIC %md ####Try It Yourself
# MAGIC 
# MAGIC Consider the following scenarios.  Write a simple block of code to answer the question(s) associated with each.  The answer to each question is provided so that your challenge is to come up with code that is logically sound and produces the required result. Code samples answering each scenario are provided by scrolling down to the bottom of the notebook.

# COMMAND ----------

# MAGIC %md ####Scenario 1
# MAGIC 
# MAGIC Using the same dataset as above, build a party classification model using an MLP Classifier algorithm as found in sklearn.neural_network. Split the data into training and test sets in a 70:30 ratio using a random see of 42.
# MAGIC 
# MAGIC Perform hyper-parameter tuning on the *alpha* parameter of the MLP Classifier.  Test values of 0.01, 0.02, etc. up to 1.0.  (HINT: Loop over values of 1 to 100 and in the loop calculate the alpha value as float(i)/100.0.) With each iteration of the alpha parameter, test whether the model accuracy improved.  Identify your best value of alpha and the accuracy achieved with it and print it to the screen.
# MAGIC 
# MAGIC Please note that you will need to set the max_iters value on the model to 1000 to allow enough cycles for convergence to occur.

# COMMAND ----------

# Scenario 1 code here

# COMMAND ----------

# MAGIC %md ####Answers (scroll down)
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p> 
# MAGIC <br></p>

# COMMAND ----------

# Scenario 1 Code
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split

# initialize best score, alpha variables
best_alpha = -1.0
best_score = -1.0

# split data into test and training sets
X_train, X_test, y_train, y_test = train_test_split(
  X, y, 
  test_size = 0.3, 
  random_state=42, 
  stratify=y
  )

# loop through alpha values
for x in range(1,101):
  
  # calculate alpha from integer
  alpha = float(x)/100.0

  # train & score model
  mlp = MLPClassifier(alpha=alpha, max_iter=1000)
  mlp.fit(X_train, y_train)
  score = mlp.score(X_test, y_test)
  
  # if this score is better than previous, record info
  if score > best_score:
    best_score = score
    best_alpha = alpha

# print results
print('Best alpha is {0} with accuracy of {1}'.format(best_alpha, best_score))